{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to Web Scrapping\n",
    "- Perian Bootcamp\n",
    "- Just keep following the commands\n",
    "- Keep making notes for following the commands\n",
    "- Keep learning the commands also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install requests\n",
    "conda install lxml\n",
    "conda install bs4\n",
    "\n",
    "# if you are not using the Anaconda Installation, you can use pip install instead of conda install, for example:\n",
    "\n",
    "pip install requests\n",
    "pip install lxml\n",
    "pip install bs4 \n",
    "\n",
    "# Now let's see what we can do with these libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Task 0 - Grabbing the title of a page\n",
    "\n",
    "Remember that this is the HTML block with the title tag. Let`s go through the main steps as listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Step 1: Use the requests library to grab the page\n",
    "# Note, this may fail if you have a firewall blocking Python/Jupyter \n",
    "# Note sometimes you need to run this twice if it fails the first time\n",
    "res = requests.get(\"http://www.example.com\")\n",
    "type(res)\n",
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "soup\n",
    "soup.select('title')\n",
    "title_tag = soup.select('title')\n",
    "title_tag[0]\n",
    "type(title_tag[0])\n",
    "title_tag[0].getText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Task 1 - Grabbing all elements of a class\n",
    "\n",
    "Let's try to grab all the section headings of the Wikipedia Article on Grace Hopper from this URL: https://en.wikipedia.org/wiki/Grace_Hopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the request\n",
    "res = requests.get('https://en.wikipedia.org/wiki/Grace_Hopper')\n",
    "\n",
    "# Create a soup from request\n",
    "soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "\n",
    "# note depending on your IP Address, \n",
    "# this class may be called something different\n",
    "soup.select(\".toctext\")\n",
    "\n",
    "for item in soup.select(\".toctext\"):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Task 2 - Getting an Image from a Website\n",
    "\n",
    "Let's attempt to grab the image of the Deep Blue Computer from this wikipedia article: https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)\")\n",
    "soup = bs4.BeautifulSoup(res.text,'lxml')\n",
    "image_info = soup.select('.thumbimage')\n",
    "image_info\n",
    "\n",
    "len(image_info)\n",
    "computer = image_info[0]\n",
    "\n",
    "type(computer)\n",
    "computer['src']\n",
    "\n",
    "image_link = requests.get('https://upload.wikimedia.org/wikipedia/commons/thumb/b/be/Deep_Blue.jpg/220px-Deep_Blue.jpg')\n",
    "\n",
    "\n",
    "# The raw content (its a binary file, meaning we will need to use binary read/write methods for saving it)\n",
    "image_link.content\n",
    "\n",
    "# Let's write this to a file:=, not the 'wb' call to denote a binary writing of the file.\n",
    "f = open('my_new_file_name.jpg','wb')\n",
    "f.write(image_link.content)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Project - Working with Multiple Pages and Items\n",
    "\n",
    "We will do the following:\n",
    "\n",
    "Figure out the URL structure to go through every page\n",
    "Scrap every page in the catalogue\n",
    "Figure out what tag/class represents the Star rating\n",
    "Filter by that star rating using an if statement\n",
    "Store the results to a list\n",
    "We can see that the URL structure is the following:\n",
    "\n",
    "http://books.toscrape.com/catalogue/page-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://books.toscrape.com/catalogue/page-{}.html'\n",
    "\n",
    "res = requests.get(base_url.format('1'))\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "\n",
    "soup.select(\".product_pod\")\n",
    "\n",
    "example.select('.star-rating.Three')\n",
    "\n",
    "\n",
    "example.select('.star-rating.Two')\n",
    "\n",
    "\n",
    "example.select('a')\n",
    "\n",
    "\n",
    "example.select('a')[1]\n",
    "\n",
    "example.select('a')[1]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "two_star_titles = []\n",
    "\n",
    "for n in range(1,51):\n",
    "\n",
    "    scrape_url = base_url.format(n)\n",
    "    res = requests.get(scrape_url)\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "    books = soup.select(\".product_pod\")\n",
    "    \n",
    "    for book in books:\n",
    "        if len(book.select('.star-rating.Two')) != 0:\n",
    "            two_star_titles.append(book.select('a')[1]['title'])\n",
    "\n",
    "two_star_titles"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
